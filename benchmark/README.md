# Snappy Benchmark Module

Benchmark suite for evaluating Spatially-Grounded Document Retrieval strategies.

Based on the research paper: [Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation](https://arxiv.org/pdf/2512.02660)

## Overview

This benchmark compares three document retrieval approaches:

1. **OCR-only**: Traditional OCR approach that returns all extracted text regions
2. **ColPali-only**: VLM-based approach that returns full page content without sub-page localization
3. **Spatial Grounding (Snappy)**: Uses interpretability maps to filter regions by query relevance

## Dataset

Uses the [BBox_DocVQA_Bench](https://huggingface.co/datasets/Yuwh07/BBox_DocVQA_Bench) dataset containing:
- 1,623 QA pairs across 80 documents
- 2,497 bounding boxes with ground truth annotations
- Categories: cs, econ, eess, math, physics, q-bio, q-fin, stat

## Metrics

The benchmark measures:

### Correctness
- **IoU (Intersection over Union)**: Overlap between predicted and ground truth bounding boxes
- **Precision/Recall/F1**: Region matching metrics at IoU threshold 0.5
- **Answer Quality**: Exact match and contains-answer rates

### Performance
- **OCR Time**: Time to extract text from image
- **Embedding Time**: Time to generate ColPali embeddings
- **Interpretability Time**: Time to compute similarity maps
- **Region Filtering Time**: Time to filter regions by relevance
- **LLM Time**: Time for answer generation
- **Total Time**: End-to-end latency

### Token Usage
- **Input Tokens**: Tokens sent to LLM
- **Output Tokens**: Tokens generated by LLM
- **Total Tokens**: Combined token usage

## Installation

```bash
# Install dependencies
pip install -r benchmark/requirements.txt

# Or install from project root
pip install -e .
```

## Usage

### Download Dataset Only

```bash
python -m benchmark --download-only
```

### Run Full Benchmark

```bash
# Run all strategies on all samples
python -m benchmark --strategies all

# Run specific strategies
python -m benchmark --strategies spatial_grounding,ocr_only

# Limit samples for quick test
python -m benchmark --strategies all --max-samples 10 --verbose
```

### Configuration Options

```bash
python -m benchmark --help

Options:
  --strategies          Strategies to run (ocr_only, colpali_only, spatial_grounding, all)
  --max-samples         Maximum samples to process
  --output-dir          Output directory for results
  --dataset-dir         Dataset cache directory
  --relevance-threshold Region relevance threshold (default: 0.3)
  --colpali-url         ColPali service URL
  --ocr-url             OCR service URL
  --llm-url             LLM service URL
  --llm-model           LLM model name
  --verbose             Enable verbose output
  --download-only       Only download dataset
```

### Environment Variables

The benchmark respects these environment variables:

```bash
COLPALI_URL=http://localhost:7000
DEEPSEEK_OCR_URL=http://localhost:8200
LLM_URL=http://localhost:8000
LLM_MODEL=gpt-4o-mini
```

## Output

Results are saved to `benchmark_results/` (or custom `--output-dir`):

- `aggregated_metrics_TIMESTAMP.json`: Summary metrics for each strategy
- `detailed_STRATEGY_TIMESTAMP.json`: Per-sample results
- `comparison_report_TIMESTAMP.md`: Markdown comparison report

### Sample Output

```
BENCHMARK COMPLETE
============================================================

spatial_grounding:
  Samples: 100 (0 errors)
  Mean IoU: 0.542
  Precision/Recall/F1: 0.678/0.512/0.584
  Mean Time: 2340.5ms
  Mean Tokens: 856
  Answer Contains Rate: 78.0%

ocr_only:
  Samples: 100 (0 errors)
  Mean IoU: 0.123
  Precision/Recall/F1: 0.156/0.891/0.265
  Mean Time: 1890.2ms
  Mean Tokens: 2456
  Answer Contains Rate: 72.0%

colpali_only:
  Samples: 100 (0 errors)
  Mean IoU: 0.098
  Precision/Recall/F1: 0.100/1.000/0.182
  Mean Time: 2156.8ms
  Mean Tokens: 2456
  Answer Contains Rate: 74.0%
```

## Architecture

```
benchmark/
├── __init__.py          # Package init
├── __main__.py          # Entry point
├── config.py            # Configuration
├── dataset.py           # Dataset loader
├── metrics.py           # Metrics computation
├── runner.py            # Main runner
├── requirements.txt     # Dependencies
└── strategies/
    ├── __init__.py
    ├── base.py              # Base strategy class
    ├── ocr_only.py          # OCR-only baseline
    ├── colpali_only.py      # ColPali-only baseline
    └── spatial_grounding.py # Snappy spatial grounding
```

## Extending

To add a new strategy:

1. Create a new file in `strategies/`
2. Inherit from `BaseStrategy`
3. Implement the `process()` method
4. Register in `strategies/__init__.py`
5. Add to `BenchmarkRunner.STRATEGY_CLASSES`

Example:

```python
from .base import BaseStrategy, StrategyResult

class MyStrategy(BaseStrategy):
    @property
    def name(self) -> str:
        return "my_strategy"

    def process(self, sample, image) -> StrategyResult:
        # Your implementation here
        pass
```

## Requirements

- Running Snappy services (ColPali, OCR, LLM)
- Python 3.10+
- ~2GB disk space for dataset
