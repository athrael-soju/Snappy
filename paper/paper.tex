\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{float}
\usepackage[hypcap=true]{caption}
\usepackage{subcaption}
\usepackage{hyperref}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\title{Spatially-Grounded Document Retrieval via\\Patch-to-Region Relevance Propagation}

\author{Agathoklis Georgiou\\
\textit{Independent Researcher}\\
\texttt{athrael.soju@gmail.com}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Late-interaction multimodal retrieval models like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. We evaluate on BBox-DocVQA with ground-truth bounding boxes. Using ColQwen3-4B with percentile-50 thresholding, our approach achieves \textbf{84.4\% hit rate at IoU@0.25}, \textbf{59.7\% at IoU@0.5}, and \textbf{35.8\% at IoU@0.7}, with mean IoU of \textbf{0.569}. Our approach reduces context tokens by \textbf{29\%} compared to returning all OCR regions and by \textbf{52\%} compared to full-page image tokens. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation at \url{https://github.com/athrael-soju/Snappy}.
\end{abstract}

\section{Introduction}

Retrieval-augmented generation (RAG) has emerged as the dominant paradigm for grounding large language models in external knowledge, enabling factual responses without costly retraining. The effectiveness of RAG systems hinges on a fundamental requirement: retrieving \emph{precisely relevant} context while minimizing noise. For text corpora, this challenge is well-studied. Dense retrievers identify semantically similar passages, and chunking strategies control context granularity. However, document collections present a fundamentally harder problem.

Documents are not sequences of tokens but \emph{spatially-organized visual artifacts}. A single page may contain heterogeneous elements, including tables, figures, equations, headers, and footnotes, each carrying distinct semantic content at different spatial locations. When a user queries ``What was the Q3 revenue?'', the answer likely resides in a specific table cell, not spread across the entire page. Yet current retrieval systems operate at the wrong granularity.

Late-interaction retrievers such as ColPali \citep{faysse2024colpali} have achieved state-of-the-art performance on document retrieval benchmarks by embedding document pages directly as images. ColPali produces 1,024 patch embeddings (a $32 \times 32$ grid) per page, each projected to 128 dimensions. The model computes relevance through late interaction, specifically a MaxSim operation that sums the maximum similarity between each query token and all document patches. This approach elegantly sidesteps OCR errors and preserves layout semantics. However, ColPali and its variants return \emph{entire pages} as retrieval units. For RAG applications, this is problematic: feeding a full page into a language model's context window introduces irrelevant content, increases latency, inflates costs, and, critically, dilutes the signal that the model must attend to. The retrieval system knows \emph{which page} contains the answer but not \emph{where on the page}.

Conversely, OCR-based pipelines extract text with precise bounding box coordinates, enabling structured representations of document content. Tables become rows and columns; figures receive captions; headers define hierarchy. This structural fidelity is invaluable for downstream processing. Yet OCR systems lack \emph{semantic grounding}. They cannot assess which extracted regions are relevant to a given query. A page with twenty OCR regions offers no ranking mechanism; all regions are treated as equally plausible candidates.

We observe that these paradigms are complementary. ColPali's patch-level similarity scores encode \emph{where} on a page the model attends when processing a query. This information is computed but discarded when returning page-level results. OCR systems know \emph{what} content exists and \emph{where} it is located, but not \emph{why} it matters. By unifying these signals through spatial coordinate mapping, we achieve region-level retrieval: returning only the document regions that are both structurally coherent (via OCR) and semantically relevant (via late-interaction attention).

Crucially, our approach operates at \emph{inference time} without additional training. Unlike RegionRAG \citep{regionrag2025}, which uses a hybrid training approach combining bounding box annotations with weakly-supervised signals from unlabeled data, our method leverages ColPali's emergent patch attention as a post-hoc spatial filter. This provides flexibility: the same approach works with any OCR system providing bounding boxes and any ColPali-family model.

\subsection{Contributions}

This paper presents a hybrid architecture for spatially-grounded document retrieval:

\begin{enumerate}
    \item \textbf{Coordinate Mapping Formalism.} We formalize the mathematical correspondence between vision transformer patch grids and OCR bounding boxes, enabling spatial alignment between heterogeneous representations (Section~\ref{sec:coord}).
    
    \item \textbf{Relevance Propagation via Interpretability Maps.} We repurpose ColPali's late interaction mechanism to generate per-query-token similarity heatmaps, then propagate these scores to OCR regions through IoU-weighted patch-region intersection (Section~\ref{sec:relevance}).
    
    \item \textbf{Two-Stage Retrieval Architecture.} We introduce a mean-pooling strategy that compresses patch embeddings along spatial axes, enabling efficient candidate retrieval before full-resolution reranking (Section~\ref{sec:twostage}).
    
    \item \textbf{Theoretical Analysis.} We establish bounds on localization precision as a function of patch resolution, derive expected context reduction and precision amplification factors, and analyze computational complexity tradeoffs (Section~\ref{sec:theory}).
    
    \item \textbf{Empirical Validation and Open Implementation.} We evaluate on BBox-DocVQA, demonstrating 59.7\% hit rate at IoU@0.5 with 52\% token savings versus full-page retrieval (Section~\ref{sec:eval}). We release Snappy, a complete open-source system implementing this architecture (Section~\ref{sec:impl}).
\end{enumerate}

Empirical evaluation on BBox-DocVQA validates that region-level retrieval achieves strong spatial grounding, with 59.7\% of retrieved regions achieving IoU $\geq$ 0.5 with ground-truth evidence bounding boxes, while reducing context tokens by over 52\% compared to full-page retrieval. These results confirm that retrieval granularity directly impacts both precision and efficiency.

\section{Background and Related Work}

\subsection{Late-Interaction Multimodal Retrieval}

\textbf{ColPali and Late Interaction.} ColPali \citep{faysse2024colpali} represents the state-of-the-art in visual document retrieval. Built on a SigLIP-So400m vision encoder, it produces 1,024 patch embeddings per page ($32 \times 32$ grid over $448 \times 448$ input resolution), each projected to 128 dimensions via a language model projection layer. Unlike single-vector approaches that pool visual features into a single embedding, ColPali preserves patch-level granularity and computes relevance through MaxSim, summing the maximum similarity between each query token and all document patches. This late interaction mechanism, inherited from ColBERT \citep{khattab2020colbert}, enables fine-grained matching while remaining computationally tractable for retrieval at scale.

The ViDoRe benchmark \citep{faysse2024colpali} evaluates visual document retrieval across diverse domains, measuring NDCG@5 for page-level retrieval. ColPali achieves strong performance, but the benchmark, like the model, operates at page granularity, leaving region-level retrieval unexplored.

\textbf{ColPali-Family Models.} The ColPali architecture has spawned a family of late-interaction visual retrievers sharing the core patch-embedding approach. ColQwen3-4B combines the ColPali framework with a Qwen3-based language model, achieving state-of-the-art performance on ViDoRe while maintaining the patch-level embeddings essential to our approach. At the efficiency frontier, ColModernVBERT \citep{teiletche2025modernvbert} is a 250M-parameter variant achieving within 0.6 NDCG@5 of ColPali with 10$\times$ fewer parameters. Our approach applies to any model in this family, as all preserve the patch-level similarity structure we exploit for region-level retrieval. We evaluate both ColQwen3-4B and ColModernVBERT to demonstrate this generality.

\subsection{Layout-Aware Document Understanding}

The LayoutLM family \citep{xu2020layoutlm, xu2021layoutlmv2, huang2022layoutlmv3} pioneered joint modeling of text, layout, and visual features for document understanding. LayoutLMv3 introduced Word-Patch Alignment (WPA) pre-training, which predicts whether image patches corresponding to text words are masked. While conceptually related to our patch-OCR alignment, WPA operates at \emph{pre-training time} to improve representations, whereas our approach uses patch similarities at \emph{inference time} for retrieval filtering. Critically, LayoutLM models are designed for document \emph{understanding} tasks (NER, classification) rather than retrieval: they lack late interaction mechanisms and query-conditioned relevance scoring.

OCR-free approaches including Donut \citep{kim2022donut} and Pix2Struct \citep{lee2023pix2struct} perform document understanding directly from pixels. UDoP \citep{tang2023udop} unifies vision, text, and layout modalities. These models excel at understanding but do not address the retrieval problem we target.

\subsection{Region-Level Document Retrieval}

\textbf{RegionRAG.} The closest existing work is RegionRAG \citep{regionrag2025}, which shifts retrieval from document-level to semantic region-level granularity. However, RegionRAG uses a hybrid approach combining bounding box annotations with weakly-supervised signals from unlabeled data and a region-level contrastive loss. Our approach is fundamentally different: we achieve region-level retrieval at \emph{inference time} using ColPali's emergent patch attention, requiring no additional training or annotation.

\textbf{DocVLM.} DocVLM \citep{docvlm2024} integrates OCR into vision-language models by compressing OCR features into learned queries. This represents the \emph{opposite direction} of our approach. DocVLM adds OCR to enhance VLM understanding, whereas we use late-interaction patch embeddings to filter and score OCR output for retrieval.

Table~\ref{tab:comparison} summarizes the positioning of our approach relative to prior work.

\begin{table}[htbp]
\centering
\caption{Comparison with related approaches. Our approach is unique in achieving region-level retrieval at inference time without additional training, by propagating late-interaction patch similarities to OCR bounding boxes.}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Granularity} & \textbf{OCR Required} & \textbf{Training} \\
\midrule
ColPali & Page-level & No & Pre-trained \\
LayoutLM & Understanding & Yes & Pre-trained \\
RegionRAG & Region-level & Yes & Hybrid supervision \\
DocVLM & Understanding & Yes & Fine-tuning \\
\textbf{Ours} & \textbf{Region-level} & \textbf{Yes} & \textbf{Inference-time only} \\
\bottomrule
\end{tabular}
\end{table}

\section{Method}

\subsection{Problem Formulation}

Given a query $q$ and a document corpus $\mathcal{D}$ where each document $d \in \mathcal{D}$ consists of one or more pages, conventional visual document retrieval returns a ranked list of pages. We reformulate the problem as \emph{region-level retrieval}: return a ranked list of (page, region) pairs where each region corresponds to a semantically coherent text block (paragraph, table, figure caption, etc.) extracted via OCR.

Let $\mathcal{P} = \{p_1, \ldots, p_N\}$ denote the set of $N$ pages in the corpus, where each page $p_i$ has an associated set of OCR regions $\mathcal{R}(p_i) = \{r_1, \ldots, r_m\}$. Each region $r_j$ is characterized by its bounding box $B(r_j) = (x_1, y_1, x_2, y_2)$ in pixel coordinates and its text content $T(r_j)$. Our goal is to compute a relevance score $\text{rel}(q, r_j)$ for each region that captures both semantic relevance and spatial grounding.

\subsection{Coordinate Mapping: Patches to Bounding Boxes}
\label{sec:coord}

ColPali encodes each page as a grid of $G \times G$ patch embeddings ($G = 32$ for ColPali-v1.3) over an input image of resolution $I \times I$ ($I = 448$). Each patch corresponds to an $s \times s$ pixel region where $s = I/G = 14$ pixels. Patches are indexed in raster scan order (left-to-right, top-to-bottom).

\begin{definition}[Patch Coordinate Mapping]
For patch index $k \in \{0, \ldots, G^2 - 1\}$, the corresponding bounding box in pixel coordinates is:
\begin{equation}
\text{patch\_bbox}(k) = (\text{col} \cdot s, \text{row} \cdot s, (\text{col} + 1) \cdot s, (\text{row} + 1) \cdot s)
\end{equation}
where $\text{row} = \lfloor k / G \rfloor$ and $\text{col} = k \mod G$.
\end{definition}

When the original document page has resolution $(W, H)$ different from the model's input resolution $I \times I$, OCR bounding boxes must be scaled to the model's coordinate space:
\begin{equation}
B'(r) = \left(x_1 \cdot \frac{I}{W}, y_1 \cdot \frac{I}{H}, x_2 \cdot \frac{I}{W}, y_2 \cdot \frac{I}{H}\right)
\end{equation}

\subsection{Relevance Propagation via Patch Similarity}
\label{sec:relevance}

Given a query $q$ tokenized into $n$ tokens with embeddings $\{q_1, \ldots, q_n\}$ and a page with patch embeddings $\{d_1, \ldots, d_m\}$ ($m = G^2 = 1{,}024$), we compute the similarity matrix:
\begin{equation}
S \in \mathbb{R}^{n \times m} \quad \text{where} \quad S_{ij} = \text{sim}(q_i, d_j)
\end{equation}
where $\text{sim}(\cdot, \cdot)$ is cosine similarity. Standard ColPali aggregates this into a page score via MaxSim:
\begin{equation}
\text{Score}_{\text{page}}(q, p) = \sum_i \max_j S_{ij}
\end{equation}

We instead extract the spatial distribution of relevance by computing a per-patch score:
\begin{equation}
\text{score}_{\text{patch}}(j) = \max_i S_{ij}
\end{equation}
This captures the maximum relevance of patch $j$ to any query token, forming a spatial heatmap over the page.

\begin{definition}[Region Relevance Score]
For OCR region $r$ with scaled bounding box $B'(r)$, we propagate patch scores via IoU-weighted aggregation:
\begin{equation}
\text{rel}(q, r) = \sum_j \text{IoU}(B'(r), \text{patch\_bbox}(j)) \cdot \text{score}_{\text{patch}}(j)
\end{equation}
where the sum is over all patches $j$ with non-zero intersection. This weights each patch's contribution by its spatial overlap with the region, ensuring that patches fully contained within the region contribute more than peripheral patches.
\end{definition}

\subsection{Two-Stage Retrieval Architecture}
\label{sec:twostage}

Computing full patch-level similarity for all pages in a large corpus is prohibitively expensive. We introduce a two-stage architecture that balances efficiency and precision.

\textbf{Stage 1: Candidate Retrieval.} We compress patch embeddings via mean pooling along spatial axes to obtain a single page-level embedding. This enables efficient approximate nearest neighbor search (via Qdrant) to retrieve top-$K$ candidate pages. Note that pooling discards spatial information, which may impact recall for pages with small relevant regions (see Section~\ref{sec:discussion}).

\textbf{Stage 2: Region Reranking.} For each candidate page, we compute full patch-level similarity and propagate scores to OCR regions as described in Section~\ref{sec:relevance}. Regions are ranked by their relevance scores, and top-$k$ regions are returned.

\subsection{Aggregation Strategies}

We consider alternative aggregation strategies for propagating patch scores to regions:

\textbf{Max Aggregation:}
\begin{equation}
\text{rel}_{\text{max}}(q, r) = \max_{j \in \text{covered}(r)} \text{score}_{\text{patch}}(j)
\end{equation}

\textbf{Mean Aggregation:}
\begin{equation}
\text{rel}_{\text{mean}}(q, r) = \frac{1}{|\text{covered}(r)|} \sum_{j \in \text{covered}(r)} \text{score}_{\text{patch}}(j)
\end{equation}

\textbf{IoU-Weighted (Default):} Uses the region relevance score from Definition 2:
\begin{equation}
\text{rel}_{\text{IoU}}(q, r) = \sum_j \text{IoU}(B'(r), \text{patch\_bbox}(j)) \cdot \text{score}_{\text{patch}}(j)
\end{equation}

The choice of aggregation strategy affects retrieval quality depending on region size and content density; we use IoU-weighted aggregation as the default based on its principled handling of partial patch overlaps.

\section{Theoretical Analysis}
\label{sec:theory}

\subsection{Precision Bounds}

The spatial precision of our approach is fundamentally bounded by patch resolution. We formalize this tradeoff.

\begin{theorem}[Localization Precision Bound]
For an OCR region with bounding box of width $w$ and height $h$ (in model coordinates), and patch size $s$, the maximum achievable localization precision is:
\begin{equation}
\text{precision} \leq \frac{w \cdot h}{(w + s) \cdot (h + s)}
\end{equation}
\end{theorem}

\begin{proof}
Consider a region with bounding box $B$ of dimensions $w \times h$. The set of patches intersecting $B$ depends on $B$'s alignment with the patch grid. In the worst case, $B$ is positioned such that it intersects partial patches on all four edges. Let the region's top-left corner fall at position $(x, y)$ within a patch. The region then spans from patch column $\lfloor x/s \rfloor$ to $\lfloor (x+w)/s \rfloor$ and from patch row $\lfloor y/s \rfloor$ to $\lfloor (y+h)/s \rfloor$. The number of intersecting patches is at most $\lceil w/s + 1 \rceil \cdot \lceil h/s + 1 \rceil$. The total area covered by these patches is at most $(w + s) \cdot (h + s)$, achieved when the region is maximally misaligned with patch boundaries. Since the region's area is $w \cdot h$, the precision (ratio of relevant area to retrieved area) is bounded by $(w \cdot h) / ((w + s) \cdot (h + s))$.
\end{proof}

\begin{corollary}
For ColPali with $s = 14$ pixels at $448 \times 448$ resolution:
\begin{itemize}
    \item A typical paragraph region ($200 \times 50$ pixels): precision $\leq 73\%$
    \item A table cell ($100 \times 30$ pixels): precision $\leq 60\%$
    \item A small label ($50 \times 20$ pixels): precision $\leq 46\%$
\end{itemize}
\end{corollary}

This analysis reveals that smaller regions suffer disproportionately from patch quantization. Applications requiring fine-grained localization (e.g., form field extraction) may benefit from higher-resolution patch grids or multi-scale approaches.

\subsection{Computational Complexity}

Let $N$ = number of pages, $M$ = average OCR regions per page, $G^2$ = patches per page, $n$ = query tokens, $d$ = embedding dimension.

\textbf{Page-level retrieval (baseline):} $O(N \cdot n \cdot G^2 \cdot d)$ for full MaxSim over all pages.

\textbf{Our two-stage approach:}
\begin{itemize}
    \item Stage 1: $O(N \cdot d)$ for ANN search with pooled embeddings
    \item Stage 2: $O(K \cdot n \cdot G^2 \cdot d + K \cdot M \cdot G^2)$ for full similarity on $K$ candidates plus region scoring
\end{itemize}

For $K \ll N$, the two-stage approach provides substantial speedup. With typical values ($N = 100{,}000$ pages, $K = 100$, $G = 32$, $n = 20$, $d = 128$, $M = 15$), Stage 1 reduces the search space by 1000$\times$ before the more expensive region-level computation.

\subsection{Expected Performance Bounds}
\label{sec:performance}

\subsubsection{Context Reduction}

Let $A_p$ denote the total page area and $A_r$ the area of the relevant region containing the answer. For a page with $M$ OCR regions of average area $\bar{A}$, we have $A_p \approx M \cdot \bar{A}$.

\begin{theorem}[Context Reduction Bound]
\label{thm:context}
Let $k$ be the number of top-scoring regions returned by our hybrid approach. The expected context reduction factor relative to page-level retrieval is:
\begin{equation}
\text{CRF} = \frac{A_p}{\sum_{i=1}^{k} A_{r_i}} \geq \frac{M}{k}
\end{equation}
with equality when all regions have equal area.
\end{theorem}

\begin{proof}
Page-level retrieval returns context proportional to $A_p$. Our approach returns context proportional to the total area of the top-$k$ regions, $\sum_{i=1}^k A_{r_i}$. Since each region has area at most $A_p$ and there are $M$ regions total, selecting $k$ regions yields area at most $k \cdot \bar{A} = k \cdot A_p / M$. The ratio $A_p / (k \cdot A_p / M) = M/k$ provides the lower bound.
\end{proof}

\begin{corollary}[Token Savings]
\label{cor:tokens}
For typical document parameters ($M = 15$ regions per page, $k = 3$ returned regions), the expected context reduction factor is at least $5\times$. This translates directly to proportional reductions in:
\begin{itemize}
    \item LLM inference cost (tokens processed)
    \item Response latency (context length)
    \item Attention dilution (irrelevant content in context window)
\end{itemize}
\end{corollary}

\subsubsection{Precision Amplification}

We model retrieval precision as the probability that a returned region contains relevant content.

\begin{definition}[Region Relevance]
Let $R_j \in \{0, 1\}$ indicate whether region $j$ contains content relevant to query $q$. For a typical factoid query, we assume $\sum_j R_j \in \{1, 2\}$ (one or two relevant regions per page).
\end{definition}

\begin{theorem}[Precision Comparison]
\label{thm:precision}
Let $\rho = \sum_j R_j / M$ denote the fraction of relevant regions on a page. Then:

\textbf{(i) OCR-only (random selection of $k$ regions):}
\begin{equation}
\mathbb{E}[\text{Precision}_{\text{OCR}}] = \rho
\end{equation}

\textbf{(ii) ColPali page-level:}
\begin{equation}
\text{Precision}_{\text{ColPali}} = \rho \quad \text{(all regions returned implicitly)}
\end{equation}

\textbf{(iii) Hybrid with score-based ranking:} If patch scores $s_j$ satisfy $\mathbb{E}[s_j | R_j = 1] > \mathbb{E}[s_j | R_j = 0]$ (relevant regions score higher on average), then selecting top-$k$ by score yields:
\begin{equation}
\mathbb{E}[\text{Precision}_{\text{Hybrid}}] > \rho
\end{equation}
with the improvement depending on the score separation between relevant and irrelevant regions.
\end{theorem}

\begin{proof}
Parts (i) and (ii) follow directly from the definition of $\rho$. For (iii), score-based ranking is equivalent to a classifier with ROC curve above the diagonal when relevant regions have higher expected scores. By the Neyman-Pearson lemma, thresholding on any statistic positively correlated with relevance improves precision over random selection.
\end{proof}

\subsubsection{Signal-to-Noise Ratio}

We formalize the intuition that region-level retrieval improves the ``signal'' available to downstream LLMs.

\begin{definition}[Retrieval SNR]
For retrieved context $C$, define:
\begin{equation}
\text{SNR}(C) = \frac{|C \cap \text{Relevant}|}{|C \cap \text{Irrelevant}|}
\end{equation}
where $|\cdot|$ denotes token count.
\end{definition}

\begin{theorem}[SNR Improvement]
\label{thm:snr}
Under the assumption that relevant and irrelevant regions have similar average token counts:

\textbf{(i)} Page-level retrieval: $\text{SNR}_{\text{page}} = \rho / (1 - \rho)$

\textbf{(ii)} Hybrid top-$k$ retrieval with precision $P_k$: $\text{SNR}_{\text{hybrid}} = P_k / (1 - P_k)$

The SNR improvement factor is:
\begin{equation}
\frac{\text{SNR}_{\text{hybrid}}}{\text{SNR}_{\text{page}}} = \frac{P_k (1 - \rho)}{\rho (1 - P_k)}
\end{equation}
\end{theorem}

\begin{proof}
For page-level retrieval, the fraction of relevant tokens equals $\rho$ by definition. For hybrid retrieval with precision $P_k$, the expected fraction of relevant tokens among returned regions is $P_k$. The SNR ratio follows from algebraic manipulation.
\end{proof}

\subsubsection{Summary of Expected Gains}

Table~\ref{tab:combined} provides a unified view of the cost-quality tradeoff, showing that our hybrid approach achieves the context efficiency of OCR-based selection while providing the relevance ranking that OCR alone cannot offer.

\begin{table}[htbp]
\centering
\caption{Combined efficiency-quality comparison. The hybrid approach uniquely achieves both low context cost and high precision. Precision values for our approach are empirically measured (IoU@0.5 on BBox-DocVQA).}
\label{tab:combined}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Context Cost} & \textbf{Precision (IoU@0.5)} & \textbf{Best Use Case} \\
\midrule
ColPali (page-level) & High (1.0$\times$) & N/A (no grounding) & Page identification \\
OCR + Random & Low (0.2$\times$) & $\sim$6.7\% (1/15 regions) & Baseline \\
\textbf{Hybrid (ours)} & \textbf{Low (0.2$\times$)} & \textbf{59.7\%} & \textbf{Precise RAG} \\
\bottomrule
\end{tabular}
\end{table}

\section{Implementation: Snappy System}
\label{sec:impl}

We implement our approach in Snappy, an open-source document retrieval system available at \url{https://github.com/athrael-soju/Snappy}.

\subsection{Architecture Overview}

Figure~\ref{fig:architecture} illustrates the Snappy system architecture, showing the parallel document indexing pipeline and query processing pipeline that converge at region-level filtering.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{architecture.png}
\caption{Snappy system architecture. The document indexing pipeline processes uploads through parallel OCR and ColPali embedding branches, storing regions in DuckDB and patch vectors in Qdrant. The query processing pipeline retrieves top-K candidates, generates interpretability maps from patch-level attention, and filters OCR regions by relevance to produce answers with spatial context.}
\label{fig:architecture}
\end{figure}

Snappy pairs a FastAPI backend with a ColPali embedding service, DeepSeek OCR for visual grounding, DuckDB analytics for metadata storage, and a Next.js frontend. The system delivers hybrid vision+text retrieval over PDFs, where each page is rasterized, embedded as multivectors, and stored alongside images and OCR-extracted text in Qdrant.

The indexing pipeline processes documents as follows: (1) render each page as an image using Poppler, (2) extract patch embeddings via the ColPali service, (3) optionally extract text regions with bounding boxes via DeepSeek OCR with visual grounding enabled, (4) store patch embeddings as multivectors and OCR metadata in Qdrant with page-level pooled embeddings for efficient retrieval, and (5) persist images to MinIO object storage for visualization.

At query time: (1) encode the query via ColPali's text encoder, (2) retrieve top-$K$ candidate pages via ANN search on pooled embeddings in Qdrant, (3) compute full patch-level similarity for candidates using the stored multivectors, (4) propagate scores to OCR regions via IoU-weighted aggregation, and (5) return ranked regions with text content, bounding boxes, and optional visual highlighting.

\subsection{Component Integration}

The ColPali embedding service runs as a dedicated container exposing patch-level embeddings over HTTP, supporting both GPU and CPU deployment. Unlike standard ColPali deployments that return only page-level scores, Snappy's integration extracts the full $(n \times G^2)$ similarity matrix stored as multivectors in Qdrant for downstream region scoring.

DeepSeek OCR provides text extraction with visual grounding capabilities. When enabled, the OCR service extracts markdown-formatted text alongside bounding box coordinates for each text region. OCR results are stored in DuckDB for SQL-based querying.

Qdrant stores both page-level pooled embeddings (for Stage 1 candidate retrieval) and full patch embeddings as multivectors (for Stage 2 region scoring).

\section{Empirical Evaluation}
\label{sec:eval}

We evaluate our approach on BBox-DocVQA, measuring spatial grounding accuracy and token efficiency. Our experiments address three questions: (1) How accurately does patch-to-region relevance propagation localize relevant content? (2) How does performance vary across document categories? (3) What token savings does the approach achieve?

\textbf{Evaluation Scope.} Our evaluation targets the core contribution: \emph{region-level localization within a page}. Given a page containing the answer, can our approach identify the specific region? This complements page-level retrieval benchmarks like ViDoRe \citep{faysse2024colpali}, which measure whether the correct page is retrieved. We use IoU-based spatial grounding metrics to directly measure localization accuracy against ground-truth evidence bounding boxes.

\subsection{Experimental Setup}

\textbf{Dataset.} BBox-DocVQA \citep{bboxdocvqa2025} provides question-answer pairs across documents from arXiv categories including computer science (cs), economics (econ), electrical engineering (eess), mathematics (math), physics, quantitative biology (q-bio), quantitative finance (q-fin), and statistics (stat). Each QA pair includes ground-truth bounding boxes marking the evidence region containing the answer. We evaluate on 1,619 samples across eight categories (1,623 total samples minus 4 that consistently failed during OCR processing and are excluded from all reported metrics).

\textbf{Models.} We evaluate two ColPali-family models: ColQwen3-4B (4B parameters), representing state-of-the-art accuracy, and ColModernVBERT (250M parameters), representing the efficiency frontier. Full results are reported for ColQwen3-4B; ColModernVBERT evaluation is ongoing with preliminary results included.

\textbf{Configuration.} We use DeepSeek OCR with visual grounding in markdown mode. For region scoring, we apply 50th-percentile thresholding with max token aggregation and max region scoring.

\textbf{Metrics.} For predicted region $B_p$ and ground-truth bounding box $B_g$, we compute:
\begin{equation}
\text{IoU}(B_p, B_g) = \frac{|B_p \cap B_g|}{|B_p \cup B_g|}
\end{equation}
We report:
\begin{itemize}
    \item \textbf{Mean IoU}: Average IoU between selected regions and ground-truth bounding boxes
    \item \textbf{Hit Rate@$\tau$}: Fraction of samples where IoU $\geq \tau$, for $\tau \in \{0.25, 0.5, 0.7\}$
    \item \textbf{Token Savings}: Percentage reduction in context tokens, computed as $(T_{\text{baseline}} - T_{\text{method}}) / T_{\text{baseline}}$, compared to (a) all OCR regions and (b) full image tokens
\end{itemize}

\subsection{Main Results}

Table~\ref{tab:main_results} presents localization accuracy.

\begin{table}[htbp]
\centering
\caption{Spatial grounding accuracy on BBox-DocVQA. Hit rates indicate the fraction of samples achieving IoU at or above the specified threshold. ColModernVBERT results are preliminary (n=19, evaluation ongoing).}
\label{tab:main_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{N} & \textbf{Mean IoU} & \textbf{IoU@0.25} & \textbf{IoU@0.5} & \textbf{IoU@0.7} \\
\midrule
ColQwen3-4B (P50) & 1619 & 0.569 & 84.4\% & 59.7\% & 35.8\% \\
ColModernVBERT (P50)$^\dagger$ & 19 & 0.373 & 57.9\% & 31.6\% & 15.8\% \\
\bottomrule
\multicolumn{6}{l}{\footnotesize $^\dagger$Preliminary results; full evaluation in progress.}
\end{tabular}
\end{table}

The results demonstrate that ColPali's patch attention effectively localizes relevant regions without additional training. With ColQwen3-4B, over 84\% of retrieved regions have meaningful overlap (IoU $\geq$ 0.25) with ground-truth evidence boxes, and 59.7\% achieve substantial overlap (IoU $\geq$ 0.5). Preliminary results with ColModernVBERT (250M parameters, 16$\times$ smaller than ColQwen3-4B) show lower localization accuracy but higher token efficiency (40\% savings vs OCR, 69\% vs full image), suggesting a quality-efficiency tradeoff; full evaluation is ongoing.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/category_performance.png}
\caption{Localization accuracy by document category. Mean IoU varies substantially, with computer science and electrical engineering achieving highest accuracy while mathematics shows lowest performance due to dense equations and small regions.}
\label{fig:category}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/token_savings.png}
\caption{Token usage comparison. Our hybrid approach reduces context tokens by 29\% vs.\ full OCR and 52\% vs.\ full image tokens.}
\label{fig:tokens}
\end{figure}

\subsection{Category Analysis}

Figure~\ref{fig:category} and Table~\ref{tab:category_results} present performance variation across document categories.

\begin{table}[htbp]
\centering
\caption{Performance by document category. Mathematics and economics documents show notably lower accuracy, likely due to denser tabular content and smaller region sizes where patch quantization effects are more pronounced.}
\label{tab:category_results}
\begin{tabular}{lrcccc}
\toprule
\textbf{Category} & \textbf{N} & \textbf{Mean IoU} & \textbf{IoU@0.25} & \textbf{IoU@0.5} & \textbf{IoU@0.7} \\
\midrule
cs & 216 & 0.697 & 94.9\% & 75.5\% & 56.9\% \\
eess & 196 & 0.656 & 94.4\% & 78.1\% & 46.9\% \\
q-bio & 176 & 0.616 & 89.8\% & 66.5\% & 42.6\% \\
physics & 213 & 0.579 & 86.9\% & 63.8\% & 34.3\% \\
stat & 200 & 0.559 & 86.5\% & 57.0\% & 31.5\% \\
q-fin & 216 & 0.543 & 86.1\% & 59.7\% & 29.2\% \\
econ & 214 & 0.513 & 75.2\% & 46.7\% & 31.8\% \\
math & 188 & 0.382 & 60.1\% & 28.7\% & 11.7\% \\
\midrule
\textbf{Overall} & \textbf{1619} & \textbf{0.569} & \textbf{84.4\%} & \textbf{59.7\%} & \textbf{35.8\%} \\
\bottomrule
\end{tabular}
\end{table}

Computer science and electrical engineering documents achieve substantially higher localization accuracy (Mean IoU $\geq$ 0.65) than economics (0.513) and mathematics (0.382) documents, with physics (0.579) and quantitative biology (0.616) falling in between. This performance gap likely reflects differences in layout complexity: economics papers frequently contain dense tables with many small cells, while mathematics papers feature equations and formulas that may span multiple regions. These smaller regions suffer disproportionately from patch quantization effects, consistent with the theoretical precision bounds in Section~\ref{sec:theory}.

\subsection{Token Efficiency}

Figure~\ref{fig:tokens} visualizes the token savings achieved by our approach. We measure context tokens that would be passed to a downstream LLM. For full image retrieval, we estimate tokens using Claude's approximation: images are resized to fit within $1568 \times 1568$ pixels while maintaining aspect ratio, then token count is computed as $\lfloor (w \times h) / 750 \rfloor$. For OCR-based methods, we tokenize extracted text using tiktoken's \texttt{cl100k\_base} encoding (GPT-4 tokenizer, used as approximation). Token counts are summed across all 1,619 evaluation samples. Table~\ref{tab:token_savings} presents token savings under different retrieval strategies.

\begin{table}[htbp]
\centering
\caption{Token efficiency comparison. Our hybrid approach with percentile-50 filtering achieves substantial savings over both returning all OCR regions and using full image tokens.}
\label{tab:token_savings}
\begin{tabular}{lrrr}
\toprule
\textbf{Method} & \textbf{Total Tokens} & \textbf{vs All OCR} & \textbf{vs Full Image} \\
\midrule
Full Image (baseline) & 4,003,039 & -- & -- \\
All OCR Regions & 2,678,723 & -- & 33.1\% \\
\textbf{Hybrid (P50, filtered)} & \textbf{1,908,329} & \textbf{28.8\%} & \textbf{52.3\%} \\
\bottomrule
\end{tabular}
\end{table}

The hybrid approach with percentile-50 filtering reduces context tokens by 29\% compared to returning all OCR regions and by 52\% compared to full-page image tokens. These savings directly translate to reduced LLM inference costs, lower latency, and decreased attention dilution in downstream RAG applications.

\section{Discussion}
\label{sec:discussion}

\subsection{Limitations}

\textbf{Patch Resolution Bound.} As analyzed in Section~\ref{sec:theory}, small regions suffer from limited localization precision due to fixed patch granularity. For ColPali's 14-pixel patches, regions smaller than approximately $35 \times 35$ pixels achieve less than 50\% precision. Future work could explore multi-scale patch embeddings or dynamic resolution based on document density.

\textbf{OCR Dependency.} Region quality depends on OCR accuracy and segmentation. Poorly segmented regions (merged paragraphs, split tables) degrade retrieval quality regardless of the retrieval model's spatial attention accuracy. Layout-aware OCR systems like DeepSeek OCR with visual grounding mitigate this, but OCR errors propagate to retrieval.

\textbf{Emergent Attention Limitations.} ColPali's patch attention is optimized for page-level retrieval through training, not explicitly for region-level localization. Attention may diffuse across semantically related but spatially distant regions (e.g., a header and its corresponding paragraph). We rely on OCR region boundaries to constrain this diffusion.

\textbf{Two-Stage Information Loss.} The mean-pooling in Stage 1 discards spatial information, potentially missing pages where relevant content is concentrated in small regions. Relevant pages could be filtered out before Stage 2 region scoring. Alternative pooling strategies (max pooling, attention-weighted pooling) may mitigate this.

\textbf{Category-Specific Performance.} Empirical evaluation reveals substantial performance variation across document types. Mathematics documents achieve only 28.7\% hit rate at IoU@0.5 compared to 78.1\% for electrical engineering documents. This gap reflects the patch quantization effects predicted by our theoretical analysis: mathematics papers contain equations and dense notation where relevant content spans small regions that are poorly aligned with the fixed patch grid. Applications targeting specific document types should consider this variation when setting performance expectations.

\subsection{Future Directions}

\textbf{Cross-Page Region Linking.} Extending region-level retrieval to link semantically related regions across pages could enable quantitative search capabilities. For example, tracking a financial metric across quarterly reports by linking table cells containing that metric across documents.

\textbf{Elimination of Image Storage.} By storing only patch embeddings and OCR-extracted text with bounding boxes, our approach potentially eliminates the need for raw image storage after indexing. This could significantly reduce infrastructure costs for large document collections while maintaining retrieval capability.

\textbf{Region-Aware Fine-Tuning.} While our approach operates without additional training, fine-tuning ColPali with region-level supervision could improve spatial attention alignment. This would sacrifice our training-free advantage but potentially yield higher localization accuracy.

\textbf{Legal Document Retrieval and Citation Grounding.} In legal contexts, retrieval precision directly impacts downstream reliability. Recent empirical work demonstrates that even retrieval-augmented legal AI tools hallucinate 17--33\% of responses, with errors compounded by coarse retrieval granularity \citep{magesh2025hallucination}. Region-level retrieval with bounding box coordinates provides citation-bounded context: each retrieved region carries verifiable provenance, constraining generation to specific, locatable sources.

\textbf{Model-Agnostic Scaling Toward Enterprise Viability.} Because our approach operates at inference time by composing outputs from independent late-interaction and OCR components, it automatically benefits from advances in either domain. Critically, the quality of region-level retrieval depends directly on the \emph{spatial precision of patch-level similarity distributions}---how tightly the model's attention concentrates on semantically relevant page regions. Our preliminary comparison between ColQwen3-4B and ColModernVBERT suggests this relationship: the larger model's more refined patch attention yields substantially higher localization accuracy (59.7\% vs 31.6\% at IoU@0.5). As late-interaction visual retrievers continue to develop more discriminative spatial attention through improved pre-training objectives and higher-resolution patch grids, our approach will inherit these gains without architectural modification. This positions the framework for enterprise deployment: organizations can upgrade their underlying retrieval model as the field advances, progressively improving retrieval precision while maintaining the same inference-time integration with existing OCR infrastructure.

\section{Conclusion}

We presented a hybrid architecture for spatially-grounded document retrieval that unifies late-interaction retrieval with structured OCR extraction. By formalizing the coordinate mapping between ColPali's patch grid and OCR bounding boxes, we enable region-level retrieval without additional training. Our approach operates entirely at inference time, providing flexibility across OCR systems and late-interaction retrieval backends.

Empirical evaluation on BBox-DocVQA validates our approach. Using ColQwen3-4B with percentile-50 thresholding, we achieve 59.7\% hit rate at IoU@0.5, demonstrating that ColPali's patch attention effectively localizes relevant regions without additional training. Token savings of 52\% compared to full-page retrieval confirm the practical efficiency gains predicted by our theoretical analysis (Theorem~\ref{thm:context}).

Category analysis across eight arXiv domains reveals systematic performance variation: computer science and electrical engineering documents achieve higher localization accuracy (Mean IoU $\geq$ 0.65) than economics (0.51) and mathematics (0.38) documents, with physics (0.58) and quantitative biology (0.62) falling in between. This gap aligns with our theoretical precision bounds: smaller regions in dense tables and equations suffer disproportionately from patch quantization effects.

The two-stage architecture balances computational efficiency with region-level granularity, making the approach practical for large-scale document collections. We release Snappy as an open-source implementation at \url{https://github.com/athrael-soju/Snappy}, demonstrating practical applicability for retrieval-augmented generation with reduced context windows and improved precision.

\section*{Acknowledgments}

The author thanks the ColPali team for their foundational work on late-interaction document retrieval and for maintaining an open research ecosystem that enabled this work. The Snappy system implementation is available at \url{https://github.com/athrael-soju/Snappy}.

\begin{thebibliography}{20}

\bibitem[Faysse et~al.(2025)]{faysse2024colpali}
Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, C{\'e}line Hudelot, and Pierre Colombo.
\newblock {ColPali}: Efficient Document Retrieval with Vision Language Models.
\newblock In \emph{The Thirteenth International Conference on Learning Representations (ICLR)}, 2025.
\newblock arXiv:2407.01449.

\bibitem[Khattab and Zaharia(2020)]{khattab2020colbert}
Omar Khattab and Matei Zaharia.
\newblock {ColBERT}: Efficient and Effective Passage Search via Contextualized Late Interaction over {BERT}.
\newblock In \emph{Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 39--48, 2020.

\bibitem[Xu et~al.(2020)]{xu2020layoutlm}
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.
\newblock {LayoutLM}: Pre-training of Text and Layout for Document Image Understanding.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 1192--1200, 2020.

\bibitem[Xu et~al.(2021)]{xu2021layoutlmv2}
Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou.
\newblock {LayoutLMv2}: Multi-modal Pre-training for Visually-rich Document Understanding.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing}, pages 2579--2591, 2021.

\bibitem[Huang et~al.(2022)]{huang2022layoutlmv3}
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei.
\newblock {LayoutLMv3}: Pre-training for Document {AI} with Unified Text and Image Masking.
\newblock In \emph{Proceedings of the 30th ACM International Conference on Multimedia}, pages 4083--4091, 2022.

\bibitem[Kim et~al.(2022)]{kim2022donut}
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
\newblock {OCR}-free Document Understanding Transformer.
\newblock In \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, pages 498--517, 2022.

\bibitem[Lee et~al.(2023)]{lee2023pix2struct}
Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.
\newblock {Pix2Struct}: Screenshot Parsing as Pretraining for Visual Language Understanding.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, pages 18893--18912, 2023.

\bibitem[Tang et~al.(2023)]{tang2023udop}
Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal.
\newblock Unifying Vision, Text, and Layout for Universal Document Processing.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19254--19264, 2023.

\bibitem[Li et~al.(2025)]{regionrag2025}
Yinglu Li, Zhiying Lu, Zhihang Liu, Yiwei Sun, Chuanbin Liu, and Hongtao Xie.
\newblock {RegionRAG}: Region-level Retrieval-Augmented Generation for Visually-Rich Documents.
\newblock arXiv:2510.27261, 2025.

\bibitem[Yu et~al.(2025)]{bboxdocvqa2025}
Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, and Jizhou Huang.
\newblock {BBox-DocVQA}: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answering.
\newblock arXiv:2511.15090, 2025.

\bibitem[Shpigel~Nacson et~al.(2024)]{docvlm2024}
Mor Shpigel~Nacson, Aviad Aberdam, Roy Ganz, Elad Ben~Avraham, Alona Golts, Yair Kittenplon, Shai Mazor, and Ron Litman.
\newblock {DocVLM}: Make Your {VLM} an Efficient Reader.
\newblock arXiv:2412.08746, 2024.

\bibitem[Teiletche et~al.(2025)]{teiletche2025modernvbert}
Paul Teiletche, Quentin Mac{\'e}, Max Conti, Antonio Loison, Gautier Viaud, Pierre Colombo, and Manuel Faysse.
\newblock {ModernVBERT}: Towards Smaller Visual Document Retrievers.
\newblock arXiv:2510.01149, 2025.

\bibitem[Magesh et~al.(2025)]{magesh2025hallucination}
Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D. Manning, and Daniel E. Ho.
\newblock Hallucination-Free? {Assessing} the Reliability of Leading {AI} Legal Research Tools.
\newblock \emph{Journal of Empirical Legal Studies}, 22:216, 2025.

\end{thebibliography}

\end{document}